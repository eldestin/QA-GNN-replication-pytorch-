{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport sys\nfrom multiprocessing import Pool\nimport spacy\nfrom spacy.matcher import Matcher\nfrom tqdm import tqdm\nimport nltk\nimport json\nimport string\nimport multiprocessing","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-04T10:15:56.448808Z","iopub.execute_input":"2022-05-04T10:15:56.450682Z","iopub.status.idle":"2022-05-04T10:16:09.381021Z","shell.execute_reply.started":"2022-05-04T10:15:56.450612Z","shell.execute_reply":"2022-05-04T10:16:09.379842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"../input/csqa-with-subgraph/csqa/grounded/train.grounded.jsonl\",\"r\") as f:\n    res = [json.loads(line) for line in f]","metadata":{"execution":{"iopub.status.busy":"2022-05-04T10:22:54.015504Z","iopub.execute_input":"2022-05-04T10:22:54.016324Z","iopub.status.idle":"2022-05-04T10:22:54.854117Z","shell.execute_reply.started":"2022-05-04T10:22:54.016249Z","shell.execute_reply":"2022-05-04T10:22:54.853243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nres[0]","metadata":{"execution":{"iopub.status.busy":"2022-05-04T10:26:40.23386Z","iopub.execute_input":"2022-05-04T10:26:40.234217Z","iopub.status.idle":"2022-05-04T10:26:40.242808Z","shell.execute_reply.started":"2022-05-04T10:26:40.234182Z","shell.execute_reply":"2022-05-04T10:26:40.241465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BLANK_STR = \"___\"\ndef rDataFramewh_word_with_blank(question_str: str):\n    # if \"What is the name of the government building that houses the U.S. Congress?\" in question_str:\n    #     print()\n    question_str = question_str.replace(\"What's\", \"What is\")\n    question_str = question_str.replace(\"whats\", \"what\")\n    question_str = question_str.replace(\"U.S.\", \"US\")\n    wh_word_offset_matches = []\n    wh_words = [\"which\", \"what\", \"where\", \"when\", \"how\", \"who\", \"why\"]\n    for wh in wh_words:\n        # Some Turk-authored SciQ questions end with wh-word\n        # E.g. The passing of traits from parents to offspring is done through what?\n\n        if wh == \"who\" and \"people who\" in question_str:\n            continue\n\n        m = re.search(wh + r\"\\?[^\\.]*[\\. ]*$\", question_str.lower())\n        if m:\n            wh_word_offset_matches = [(wh, m.start())]\n            break\n        else:\n            # Otherwise, find the wh-word in the last sentence\n            m = re.search(wh + r\"[ ,][^\\.]*[\\. ]*$\", question_str.lower())\n            if m:\n                wh_word_offset_matches.append((wh, m.start()))\n            # else:\n            #     wh_word_offset_matches.append((wh, question_str.index(wh)))\n\n    # If a wh-word is found\n    if len(wh_word_offset_matches):\n        # Pick the first wh-word as the word to be replaced with BLANK\n        # E.g. Which is most likely needed when describing the change in position of an object?\n        wh_word_offset_matches.sort(key=lambda x: x[1])\n        wh_word_found = wh_word_offset_matches[0][0]\n        wh_word_start_offset = wh_word_offset_matches[0][1]\n        # Replace the last question mark with period.\n        question_str = re.sub(r\"\\?$\", \".\", question_str.strip())\n        # Introduce the blank in place of the wh-word\n        fitb_question = (question_str[:wh_word_start_offset] + BLANK_STR +\n                         question_str[wh_word_start_offset + len(wh_word_found):])\n        # Drop \"of the following\" as it doesn't make sense in the absence of a multiple-choice\n        # question. E.g. \"Which of the following force ...\" -> \"___ force ...\"\n        final = fitb_question.replace(BLANK_STR + \" of the following\", BLANK_STR)\n        final = final.replace(BLANK_STR + \" of these\", BLANK_STR)\n        return final\n\n    elif \" them called?\" in question_str:\n        return question_str.replace(\" them called?\", \" \" + BLANK_STR + \".\")\n    elif \" meaning he was not?\" in question_str:\n        return question_str.replace(\" meaning he was not?\", \" he was not \" + BLANK_STR + \".\")\n    elif \" one of these?\" in question_str:\n        return question_str.replace(\" one of these?\", \" \" + BLANK_STR + \".\")\n    elif re.match(r\".*[^\\.\\?] *$\", question_str):\n        # If no wh-word is found and the question ends without a period/question, introduce a\n        # blank at the end. e.g. The gravitational force exerted by an object depends on its\n        return question_str + \" \" + BLANK_STR\n    else:\n        # If all else fails, assume \"this ?\" indicates the blank. Used in Turk-authored questions\n        # e.g. Virtually every task performed by living organisms requires this?\n        return re.sub(r\" this[ \\?]\", \" ___ \", question_str)\n\n# Get a Fill-In-The-Blank (FITB) statement from the question text. E.g. \"George wants to warm his\n# hands quickly by rubbing them. Which skin surface will produce the most heat?\" ->\n# \"George wants to warm his hands quickly by rubbing them. ___ skin surface will produce the most\n# heat?\ndef get_fitb_from_question(question_text: str) -> str:\n    '''\n    This function first find the wh word, then change the multiple choice to\n    fill_question. \n    '''\n    fitb = replace_wh_word_with_blank(question_text)\n    if not re.match(\".*_+.*\", fitb):\n        # print(\"Can't create hypothesis from: '{}'. Appending {} !\".format(question_text, BLANK_STR))\n        # Strip space, period and question mark at the end of the question and add a blank\n        fitb = re.sub(r\"[\\.\\? ]*$\", \"\", question_text.strip()) + \" \" + BLANK_STR\n    return fitb\n\ndef create_hypothesis(mdf_q:str, choice:str, ans_pos:bool) -> str:\n    '''\n    This function create the mapping string and it's span\n    '''\n    if \".\"+ BLANK_STR in mdf_q or mdf_q.startswith(BLANK_STR):\n        # means bof(begin of a sentence), makes it upper\n        choice = choice[0].upper() + choice[1:]\n    else:\n        choice = choice.lower()\n    # remove . if question not end with ___\n    if not mdf_q.endswith(BLANK_STR):\n        choice = choice.rstrip(\".\")\n    if not ans_pos:\n        try:\n            hypothesis = re.sub(\"__+\", choice, mdf_q)\n        except:\n            print(choice, mdf_q)\n        return hypothesis\n    choice = choice.strip()\n    m = re.search(\"__+\", mdf_q)\n    start = m.start()\n    # substitute __+ to choice in mdf_q\n    length = (len(choice) - 1) if mdf_q.endswith(BLANK_STR) and choice[-1] in ['.', '?', '!'] else len(choice)\n    hypothesis = re.sub(\"__+\", choice, mdf_q)\n    # return the question text with answer fill in those blank, and return the span index\n    return hypothesis, (start, start + length)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T15:59:22.718039Z","iopub.execute_input":"2022-05-03T15:59:22.718268Z","iopub.status.idle":"2022-05-03T15:59:22.742799Z","shell.execute_reply.started":"2022-05-03T15:59:22.718241Z","shell.execute_reply":"2022-05-03T15:59:22.741983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_Path = \"../input/csqa-with-subgraph/csqa/\"\ndef create_output_dict(input_json: dict, statement: str, label: bool, ans_pos: bool, pos=None) -> dict:\n    if \"statements\" not in input_json:\n        input_json[\"statements\"] = []\n    if not ans_pos:\n        input_json[\"statements\"].append({\"label\": label, \"statement\": statement})\n    else:\n        input_json[\"statements\"].append({\"label\": label, \"statement\": statement, \"ans_pos\": pos})\n    return input_json\ndef convert_qajson(qa_file:str, op_file: str,ans_pos:bool=False):\n    print(\"Start converting\")\n    nrow = sum(1 for _ in open(qa_file,'r'))\n    with open(qa_file,\"r\") as f, open(op_file, \"w\") as op:\n        for line in tqdm(f, total = nrow):\n            json_l = json.loads(line)\n            question_text = json_l['question']['stem']\n            choices = json_l[\"question\"][\"choices\"]\n            for choice in choices:\n                choice_text = choice[\"text\"]\n                pos = None\n                if not ans_pos:\n                    statement = create_hypothesis(get_fitb_from_question(question_text), choice_text, ans_pos)\n                else:\n                    statement, pos = create_hypothesis(get_fitb_from_question(question_text), choice_text, ans_pos)\n                create_output_dict(json_l,statement,choice[\"label\"] == json_l.get(\"answerKey\", \"A\"), ans_pos, pos)\n            op.write(json.dumps(json_l))\n            op.write(\"\\n\")\n    print(\"Done\")","metadata":{"execution":{"iopub.status.busy":"2022-05-03T15:59:22.743972Z","iopub.execute_input":"2022-05-03T15:59:22.744591Z","iopub.status.idle":"2022-05-03T15:59:22.759236Z","shell.execute_reply.started":"2022-05-03T15:59:22.74455Z","shell.execute_reply":"2022-05-03T15:59:22.758612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"convert_qajson(\"../input/csqa-with-subgraph/csqa/train_rand_split.jsonl\",\"test.json\")","metadata":{"execution":{"iopub.status.busy":"2022-05-03T15:59:22.761271Z","iopub.execute_input":"2022-05-03T15:59:22.761555Z","iopub.status.idle":"2022-05-03T15:59:24.814071Z","shell.execute_reply.started":"2022-05-03T15:59:22.761505Z","shell.execute_reply":"2022-05-03T15:59:24.811945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"./test.json\") as f:\n    ls = [line for line in f]","metadata":{"execution":{"iopub.status.busy":"2022-05-03T15:59:24.816006Z","iopub.execute_input":"2022-05-03T15:59:24.816429Z","iopub.status.idle":"2022-05-03T15:59:24.847518Z","shell.execute_reply.started":"2022-05-03T15:59:24.816383Z","shell.execute_reply":"2022-05-03T15:59:24.846665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls[-1]","metadata":{"execution":{"iopub.status.busy":"2022-05-03T15:59:24.848994Z","iopub.execute_input":"2022-05-03T15:59:24.849653Z","iopub.status.idle":"2022-05-03T15:59:24.861349Z","shell.execute_reply.started":"2022-05-03T15:59:24.849606Z","shell.execute_reply":"2022-05-03T15:59:24.860291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ground result and extract subgraph","metadata":{}},{"cell_type":"code","source":"blacklist = set([\"-PRON-\", \"actually\", \"likely\", \"possibly\", \"want\",\n                 \"make\", \"my\", \"someone\", \"sometimes_people\", \"sometimes\", \"would\", \"want_to\",\n                 \"one\", \"something\", \"sometimes\", \"everybody\", \"somebody\", \"could\", \"could_be\"\n                 ])\nnltk.download('stopwords', quiet=True)\nnltk_stopwords = nltk.corpus.stopwords.words('english')\nCPNET_VOCAB = None\nPATTERN_PATH = None\nnlp = None\nmatcher = None","metadata":{"execution":{"iopub.status.busy":"2022-05-03T15:59:24.862995Z","iopub.execute_input":"2022-05-03T15:59:24.864111Z","iopub.status.idle":"2022-05-03T15:59:25.034713Z","shell.execute_reply.started":"2022-05-03T15:59:24.864059Z","shell.execute_reply":"2022-05-03T15:59:25.033482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_matcher(nlp, pattern_path):\n    with open(pattern_path, \"r\", encoding=\"utf8\") as fin:\n        all_patterns = json.load(fin)\n    matcher = Matcher(nlp.vocab)\n    for concept, pattern in all_patterns.items():\n        matcher.add(concept,[pattern])\n    return matcher\ndef load_cpnet_vocab(cpnet_vocab_path):\n    '''\n    This function load the vocab path\n    '''\n    with open(cpnet_vocab_path, \"r\", encoding=\"utf8\") as fin:\n        cpnet_vocab = [l.strip() for l in fin]\n    cpnet_vocab = [c.replace(\"_\", \" \") for c in cpnet_vocab]\n    return cpnet_vocab\ndef lemmatize(nlp, concept):\n\n    doc = nlp(concept.replace(\"_\", \" \"))\n    lcs = set()\n    lcs.add(\"_\".join([token.lemma_ for token in doc]))  # all lemma\n    return lcs\ndef match_mentioned_concepts(sents, answers, num_processes):\n    '''\n    Use multi process for a function\n    '''\n    res = []\n    with Pool(num_processes) as p:\n        res = list(tqdm(p.imap(ground_qa_pair, zip(sents, answers)), total=len(sents)))\n    return res\ndef ground_qa_pair(qa_pair):\n    '''\n    This function create a pipeline and \n    detect all the concepts in question and answer\n    Also, if not concepts are extracted, will set some default concept, which is called hard concepts here.\n    '''\n    global nlp, matcher\n    # ---------------------------- initialize\n    if nlp is None or matcher is None:\n        #print(\"start loading info\")\n        nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser', 'textcat'])\n        nlp.add_pipe('sentencizer')\n        matcher = load_matcher(nlp, PATTERN_PATH)\n        print(\"done loading matcher\")\n    #print(\"Start extract concepts\")\n    s, a = qa_pair\n    # --------------------------- extract concepts -----------------------\n    all_concepts = ground_mentioned_concepts(nlp, matcher, s, a)\n    answer_concepts = ground_mentioned_concepts(nlp, matcher, a)\n    question_concepts = all_concepts - answer_concepts\n    # -------------------------- special cases ---------------------------\n    if len(question_concepts) == 0:\n        question_concepts = hard_ground(nlp, s, CPNET_VOCAB)  # not very possible\n\n    if len(answer_concepts) == 0:\n        answer_concepts = hard_ground(nlp, a, CPNET_VOCAB)  # some case\n        \n    question_concepts = sorted(list(question_concepts))\n    answer_concepts = sorted(list(answer_concepts))\n    return {\"sent\": s, \"ans\": a, \"qc\": question_concepts, \"ac\": answer_concepts}\n# extract concept, which is the basic part of this project:\ndef ground_mentioned_concepts(nlp, matcher, s, ans=None):\n    '''\n    nlp is the pipeline operation\n    matcher is the match dic\n    '''\n    s = s.lower()\n    doc = nlp(s)\n    matches = matcher(doc)\n    mentioned_concepts = set()\n    span_to_concepts = {}\n    \n    if ans is not None:\n        # means we have teh answer\n        ans_matcher = Matcher(nlp.vocab)\n        ans_words = nlp(ans)\n        # print(ans_words)\n        tmp = [[{'TEXT': token.text.lower()}] for token in ans_words]\n        ans_matcher.add(ans, tmp)\n\n        ans_match = ans_matcher(doc)\n        ans_mentions = set()\n        for _, ans_start, ans_end in ans_match:\n            ans_mentions.add((ans_start, ans_end))\n    for match_id, start, end in matches:\n        if ans is not None:\n            if (start, end) in ans_mentions:\n                continue\n        # get the match part\n        span = doc[start:end].text\n        # get source concept\n        original_concept = nlp.vocab.strings[match_id]\n        original_concept_set = set()\n        original_concept_set.add(original_concept)\n        \n        # lemmatize \n        if len(original_concept.split(\"_\")) == 1:\n            # tag = doc[start].tag_\n            # if tag in ['VBN', 'VBG']:\n            original_concept_set.update(lemmatize(nlp, nlp.vocab.strings[match_id]))\n\n        if span not in span_to_concepts:\n            span_to_concepts[span] = set()\n        span_to_concepts[span].update(original_concept_set)\n    for span, concepts in span_to_concepts.items():\n        concepts_sorted = list(concepts)\n        concepts_sorted.sort(key=len)\n        shortest = concepts_sorted[0:3]\n\n        for c in shortest:\n            if c in blacklist:\n                continue\n            # a set with one string like: set(\"like_apples\")\n            lcs = lemmatize(nlp, c)\n            intersect = lcs.intersection(shortest)\n            if len(intersect) > 0:\n                mentioned_concepts.add(list(intersect)[0])\n            else:\n                mentioned_concepts.add(c)\n\n        # if a mention exactly matches with a concept\n        exact_match = set([concept for concept in concepts_sorted if concept.replace(\"_\", \" \").lower() == span.lower()])\n        assert len(exact_match) < 2\n        mentioned_concepts.update(exact_match)\n\n    return mentioned_concepts\ndef hard_ground(nlp, sent, cpnet_vocab):\n    sent = sent.lower()\n    doc = nlp(sent)\n    res = set()\n    for t in doc:\n        if t.lemma_ in cpnet_vocab:\n            res.add(t.lemma_)\n    sent = \" \".join([t.text for t in doc])\n    if sent in cpnet_vocab:\n        res.add(sent)\n    try:\n        assert len(res) > 0\n    except Exception:\n        print(f\"for {sent}, concept not found in hard grounding.\")\n    return res\ndef prune(data, cpnet_vocab_path):\n    # reload cpnet_vocab\n    with open(cpnet_vocab_path, \"r\", encoding=\"utf8\") as fin:\n        cpnet_vocab = [l.strip() for l in fin]\n\n    prune_data = []\n    for item in tqdm(data):\n        qc = item[\"qc\"]\n        prune_qc = []\n        for c in qc:\n            if c[-2:] == \"er\" and c[:-2] in qc:\n                continue\n            if c[-1:] == \"e\" and c[:-1] in qc:\n                continue\n            have_stop = False\n            # remove all concepts having stopwords, including hard-grounded ones\n            for t in c.split(\"_\"):\n                if t in nltk_stopwords:\n                    have_stop = True\n            if not have_stop and c in cpnet_vocab:\n                prune_qc.append(c)\n\n        ac = item[\"ac\"]\n        prune_ac = []\n        for c in ac:\n            if c[-2:] == \"er\" and c[:-2] in ac:\n                continue\n            if c[-1:] == \"e\" and c[:-1] in ac:\n                continue\n            all_stop = True\n            for t in c.split(\"_\"):\n                if t not in nltk_stopwords:\n                    all_stop = False\n            if not all_stop and c in cpnet_vocab:\n                prune_ac.append(c)\n\n        try:\n            assert len(prune_ac) > 0 and len(prune_qc) > 0\n        except Exception as e:\n            pass\n        item[\"qc\"] = prune_qc\n        item[\"ac\"] = prune_ac\n\n        prune_data.append(item)\n    return prune_data\ndef ground(statement_path, cpnet_vocab_path, pattern_path, output_path, num_processes=1):\n    global PATTERN_PATH, CPNET_VOCAB\n    if PATTERN_PATH is None:\n        PATTERN_PATH = pattern_path\n        CPNET_VOCAB = load_cpnet_vocab(cpnet_vocab_path)\n    sents, answers = [], []\n    with open(statement_path, \"r\") as fin:\n        lines = [line for line in fin]\n    for line in lines:\n        # for each question json file\n        if line == \"\":\n            continue\n        j = json.loads(line)\n        #print(j)\n        # {'answerKey': 'B',\n        #   'id': 'b8c0a4703079cf661d7261a60a1bcbff',\n        #   'question': {'question_concept': 'magazines',\n        #                 'choices': [{'label': 'A', 'text': 'doctor'}, {'label': 'B', 'text': 'bookstore'}, {'label': 'C', 'text': 'market'}, {'label': 'D', 'text': 'train station'}, {'label': 'E', 'text': 'mortuary'}],\n        #                 'stem': 'Where would you find magazines along side many other printed works?'},\n        #   'statements': [{'label': False, 'statement': 'Doctor would you find magazines along side many other printed works.'}, {'label': True, 'statement': 'Bookstore would you find magazines along side many other printed works.'}, {'label': False, 'statement': 'Market would you find magazines along side many other printed works.'}, {'label': False, 'statement': 'Train station would you find magazines along side many other printed works.'}, {'label': False, 'statement': 'Mortuary would you find magazines along side many other printed works.'}]}\n        for statement in j[\"statements\"]:\n            sents.append(statement[\"statement\"])\n            \n        for answer in j[\"question\"][\"choices\"]:\n            ans = answer[\"text\"]\n            answers.append(ans)\n    res = match_mentioned_concepts(sents, answers, num_processes)\n    res = prune(res, cpnet_vocab_path)\n\n        # check_path(output_path)\n    with open(output_path, 'w') as fout:\n        for dic in res:\n            fout.write(json.dumps(dic) + '\\n')\n\n    print(f'grounded concepts saved to {output_path}')\n    print()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-03T16:11:41.542526Z","iopub.execute_input":"2022-05-03T16:11:41.543115Z","iopub.status.idle":"2022-05-03T16:11:41.605762Z","shell.execute_reply.started":"2022-05-03T16:11:41.543072Z","shell.execute_reply":"2022-05-03T16:11:41.604957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ground(\"../input/csqa-with-subgraph/csqa/statement/train.statement.jsonl\", \"../input/cp-net-en/CPnet_en/concept.txt\", \"../input/cp-net-en/CPnet_en/matcher_patterns.json\", \"a.json\",4)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T16:11:42.168361Z","iopub.execute_input":"2022-05-03T16:11:42.169064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}