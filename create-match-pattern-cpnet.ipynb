{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from multiprocessing import Pool\nimport spacy\nfrom spacy.matcher import Matcher\nfrom tqdm import tqdm\nimport nltk\nimport json\nimport string\nimport multiprocessing","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-03T06:37:39.53741Z","iopub.execute_input":"2022-05-03T06:37:39.537681Z","iopub.status.idle":"2022-05-03T06:37:39.542595Z","shell.execute_reply.started":"2022-05-03T06:37:39.537654Z","shell.execute_reply":"2022-05-03T06:37:39.541765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"blacklist = set([\"-PRON-\", \"actually\", \"likely\", \"possibly\", \"want\",\n                 \"make\", \"my\", \"someone\", \"sometimes_people\", \"sometimes\", \"would\", \"want_to\",\n                 \"one\", \"something\", \"sometimes\", \"everybody\", \"somebody\", \"could\", \"could_be\"\n                 ])\nnltk.download('stopwords', quiet=True)\nnltk_stopwords = nltk.corpus.stopwords.words('english')\nCPNET_VOCAB = None\nPATTERN_PATH = None\nnlp = None\nmatcher = None","metadata":{"execution":{"iopub.status.busy":"2022-05-03T06:11:20.429961Z","iopub.execute_input":"2022-05-03T06:11:20.430271Z","iopub.status.idle":"2022-05-03T06:11:20.436735Z","shell.execute_reply.started":"2022-05-03T06:11:20.430235Z","shell.execute_reply":"2022-05-03T06:11:20.436174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_cpnet_vocab(cpnet_vocab_path):\n    '''\n    This function load the vocab path\n    '''\n    with open(cpnet_vocab_path, \"r\", encoding=\"utf8\") as fin:\n        cpnet_vocab = [l.strip() for l in fin]\n    cpnet_vocab = [c.replace(\"_\", \" \") for c in cpnet_vocab]\n    return cpnet_vocab\nvocab = load_cpnet_vocab(\"../input/cp-net-en/CPnet_en/concept.txt\")","metadata":{"execution":{"iopub.status.busy":"2022-05-03T06:11:59.249835Z","iopub.execute_input":"2022-05-03T06:11:59.25013Z","iopub.status.idle":"2022-05-03T06:11:59.598282Z","shell.execute_reply.started":"2022-05-03T06:11:59.250098Z","shell.execute_reply":"2022-05-03T06:11:59.597661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_pattern(nlp, doc, debug = False):\n    pronoun_list = set([\"my\", \"you\", \"it\", \"its\", \"your\", \"i\", \"he\", \"she\", \"his\", \"her\", \"they\", \"them\", \"their\", \"our\", \"we\"])\n    if len(doc) >=5 or doc[0].text in pronoun_list or doc[-1].text in pronoun_list or \\\n        all([(token.text in nltk_stopwords or token.lemma_ in nltk_stopwords or token.lemma_ in blacklist) for token in doc]):\n        # ignore those concepts become pattern\n        return None\n    pattern = []\n    for token in doc:\n        # lemmatize the type\n        pattern.append({\"LEMMA\":token.lemma_})\n    return pattern\n\ndef create_matcher_pattern(cpnet_vocab_path, output_path, debug = False):\n    '''\n    This function create the matcher patterns\n    '''\n    cpnet_vocab = load_cpnet_vocab(cpnet_vocab_path)\n    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner', 'textcat'])\n    docs = nlp.pipe(cpnet_vocab)\n    all_patterns = {}\n    for doc in tqdm(docs, total = len(cpnet_vocab)):\n        pattern = create_pattern(nlp,doc,debug)\n        if pattern is None:\n            continue\n        # create mapping\n        all_patterns[\"_\".join(doc.text.split(\" \"))] = pattern\n    with open(output_path, \"w\", encoding=\"utf8\") as fout:\n        json.dump(all_patterns, fout)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T06:35:49.250987Z","iopub.execute_input":"2022-05-03T06:35:49.251641Z","iopub.status.idle":"2022-05-03T06:35:49.263556Z","shell.execute_reply.started":"2022-05-03T06:35:49.251598Z","shell.execute_reply":"2022-05-03T06:35:49.262743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cores = multiprocessing.cpu_count()\n# pool = multiprocessing.Pool(processes=cores)\n# pool.map(create_matcher_pattern,kwargs = {\"cpnet_vocab_path\":\"../input/cp-net-en/CPnet_en/concept.txt\", \"output_path\":\"./\"})\ncreate_matcher_pattern(\"../input/cp-net-en/CPnet_en/concept.txt\", \"file.json\")","metadata":{"execution":{"iopub.status.busy":"2022-05-03T06:51:14.186661Z","iopub.execute_input":"2022-05-03T06:51:14.186928Z","iopub.status.idle":"2022-05-03T06:56:50.754817Z","shell.execute_reply.started":"2022-05-03T06:51:14.186899Z","shell.execute_reply":"2022-05-03T06:56:50.754058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}